{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re,csv, os, itertools, pandas as pd,docx2txt\n",
    "from tqdm import tqdm\n",
    "from pattern.web import PDF\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from spacy.lang.id import Indonesian\n",
    "from html import unescape\n",
    "from unidecode import unidecode\n",
    "from bz2 import BZ2File as bz2\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "\n",
    "def LoadStopWords(lang):\n",
    "    L = lang.lower().strip()\n",
    "    if L == 'en' or L == 'english' or L == 'inggris':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stops =  set([t.strip() for t in LoadDocuments(file = 'C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/stopwords_eng.txt')[0]])\n",
    "    elif L == 'id' or L == 'indonesia' or L=='indonesian':\n",
    "        lemmatizer = Indonesian() \n",
    "        stops = set([t.strip() for t in LoadDocuments(file = 'C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/stopwords_id.txt')[0]])\n",
    "    else:\n",
    "        print('Warning, language not recognized. Empty StopWords Given')\n",
    "        stops = set(); lemmatizer = None\n",
    "    return stops, lemmatizer\n",
    "\n",
    "def fixTags(T):\n",
    "    getHashtags = re.compile(r\"#(\\w+)\")\n",
    "    pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "    t = T\n",
    "    tagS = re.findall(getHashtags, T)\n",
    "    for tag in tagS:\n",
    "        proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "        t = t.replace('#'+tag,proper_words)\n",
    "    return t\n",
    "\n",
    "def readBz2(file):\n",
    "    with bz2(file, \"r\") as bzData:\n",
    "        txt = []\n",
    "        for line in bzData:\n",
    "            try:\n",
    "                txt.append(line.strip().decode('utf-8','replace'))\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(txt)\n",
    "\n",
    "def LoadDocuments(dPath=None,types=None, file = None): # types = ['pdf','doc','docx','txt','bz2']\n",
    "    Files, Docs = [], []\n",
    "    if types:\n",
    "        for tipe in types:\n",
    "            Files += crawlFiles(dPath,tipe)\n",
    "    if file:\n",
    "        Files = [file]\n",
    "    if not types and not file: # get all files regardless of their extensions\n",
    "        Files += crawlFiles(dPath)\n",
    "    for f in Files:\n",
    "        if f[-3:].lower()=='pdf':\n",
    "            try:\n",
    "                Docs.append(PDF(f).string)\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='txt' or f[-3:].lower()=='dic':\n",
    "            try:\n",
    "                df=open(f,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "                Docs.append(df.readlines());df.close()\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='bz2':\n",
    "            try:\n",
    "                Docs.append(readBz2(f))\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-4:].lower()=='docx':\n",
    "            try:\n",
    "                Docs.append(docx2txt.process(f))\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='csv':\n",
    "            Docs.append(pd.read_csv(f))\n",
    "        else:\n",
    "            print('Unsupported format {0}'.format(f))\n",
    "    if file:\n",
    "        Docs = Docs[0]\n",
    "    return Docs, Files\n",
    "\n",
    "def DelPic(text): #untuk menghilangkan informasi gambar\n",
    "    D = text.split()\n",
    "    D = [d for d in D if 'pic.twitter.com' not in d]\n",
    "    return ' ' .join(D)\n",
    "\n",
    "def LoadSlang(DirSlang):\n",
    "    Slangs =LoadDocuments(file = DirSlang)\n",
    "    SlangDict={}\n",
    "    for slang in Slangs[0]:\n",
    "        try:\n",
    "            key, value = slang.split(':')\n",
    "            SlangDict[key.strip()] = value.strip()\n",
    "        except:\n",
    "            pass\n",
    "    return SlangDict\n",
    "\n",
    "#POS Tagging\n",
    "from nltk.tag import CRFTagger\n",
    "def postag(text):\n",
    "    #Tokenisasi Data\n",
    "    tokenized_sents = word_tokenize(text)\n",
    "    #pemberian Tag tiap token\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file('C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/CRFTagger-1.0/CRFTagger/model/model.txt') \n",
    "    #directorynya disesuaikan meletakan file crfnya, harus download dlu file crfnya\n",
    "    pt = ct.tag(tokenized_sents)\n",
    "    ptN = []\n",
    "    noun = set(['NN','NNP', 'NNS','NNPS'])\n",
    "    tmp = []\n",
    "    for w in pt:\n",
    "        if w[1] in noun:\n",
    "            tmp.append(w[0])\n",
    "    if len(tmp)>0:\n",
    "        ptN.append(' '.join(tmp))\n",
    "    return ' '.join(ptN)\n",
    "\n",
    "def cleanText(T, fix={}, lang = 'id', lemma=None, stops = set(), symbols_remove = False, min_charLen = 0): \n",
    "    # lang & stopS only 2 options : 'en' atau 'id'\n",
    "    # symbols ASCII atau alnum\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',T) #remove urls if any\n",
    "    t = DelPic(t)\n",
    "    t = unescape(t) # html entities fix\n",
    "    t = fixTags(t) # fix abcDef\n",
    "    t = t.lower().strip() # lowercase\n",
    "    t = unidecode(t)\n",
    "    t = ''.join(''.join(s)[:2] for _, s in itertools.groupby(t)) # remove repetition\n",
    "    t = sent_tokenize(t) # sentence segmentation. String to list\n",
    "    for i, K in enumerate(t):\n",
    "        if symbols_remove:\n",
    "            K = re.sub(r'[^.,a-zA-Z0-9 \\n\\.]',' ',K)\n",
    "        \n",
    "        cleanList = []\n",
    "        if lang =='en':\n",
    "            listKata = word_tokenize(K) # word tokenize\n",
    "            for token in listKata:\n",
    "                if token in fix.keys():\n",
    "                    token = fix[token]\n",
    "                if lemma:\n",
    "                    token = lemma.lemmatize(token)\n",
    "                if stops:\n",
    "                    if len(token)>=min_charLen and token not in stops:\n",
    "                        cleanList.append(token)\n",
    "                else:\n",
    "                    if len(token)>=min_charLen:\n",
    "                        cleanList.append(token)\n",
    "            t[i] = ' '.join(cleanList)\n",
    "        else:\n",
    "            if lemma:\n",
    "                K = lemma(K)\n",
    "                listKata = [token.text for token in K]\n",
    "            else:\n",
    "                listKata = TextBlob(K).words\n",
    "                \n",
    "            for token in listKata:\n",
    "                if token in fix.keys():\n",
    "                    token = fix[token]\n",
    "                \n",
    "                if lemma:\n",
    "                    token = lemma(token)[0].lemma_\n",
    "                if stops:    \n",
    "                    if len(token)>=min_charLen and token not in stops:\n",
    "                        cleanList.append(token)\n",
    "                else:\n",
    "                    if len(token)>=min_charLen:\n",
    "                        cleanList.append(token)\n",
    "            t[i] = ' '.join(cleanList)\n",
    "    return ' '.join(t) \n",
    "\n",
    "stops, lemmatizer = LoadStopWords(lang='en')\n",
    "Slangs=LoadSlang( 'C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/slang.txt')\n",
    "\n",
    "def predictAspek(text):\n",
    "      ##Preprocessing\n",
    "      text = cleanText(text,Slangs, lemma=lemmatizer,lang='en', stops = stops, symbols_remove = True, min_charLen =3)\n",
    "      text = cleanText(text,fix={}, lemma=lemmatizer,lang='en', stops = stops, symbols_remove = True, min_charLen =3)\n",
    "\n",
    "      ##Load Vectorized\n",
    "      tfidf = pickle.load(open(\"tfidf.pkl\", \"rb\"))\n",
    "#       print(tfidf)\n",
    "      test = tfidf.transform([text,''])\n",
    "\n",
    "      ##Predict\n",
    "      filename = 'OvR_SVM.pkl'\n",
    "      clf = pickle.load(open(filename, \"rb\"))\n",
    "      label = clf.predict(test[0])\n",
    "      return label[0]\n",
    "    \n",
    "def predictSent(text): # I give label to all data review,= before splitting, and then I saved the tfidf words and model for both aspect and sentiment classification, so after splitting later, label for sentence that has been splitting are given by model \n", 
    "      ##Preprocessing\n",
    "      text = cleanText(text,Slangs, lemma=lemmatizer,lang='en', stops = stops, symbols_remove = True, min_charLen =3)\n",
    "      text = cleanText(text,fix={}, lemma=lemmatizer,lang='en', stops = stops, symbols_remove = True, min_charLen =3)\n",
    "\n",
    "      ##Load Vectorized\n",
    "      tfidf = pickle.load(open(\"tfidf.pkl\", \"rb\"))\n",
    "#       print(tfidf)\n",
    "      test = tfidf.transform([text,''])\n",
    "\n",
    "      ##Predict\n",
    "      filename = 'bnb_sentimen.pkl'\n",
    "      clf = pickle.load(open(filename, \"rb\"))\n",
    "      label = clf.predict(test[0])\n",
    "      return label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/tokped_bersih3_label aspek 3.xlsx')\n",
    "listReview=data['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>aspek</th>\n",
       "      <th>Review</th>\n",
       "      <th>Cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>helpful</td>\n",
       "      <td>Recomended app</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>I have relied on Tokopedia to obtain from chea...</td>\n",
       "      <td>relied cheap mundane item pricy gadget extra c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>What's with OVO shovel treatment? Dissapointed...</td>\n",
       "      <td>ovo shovel treatment dissapointed narrow money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>tampilan</td>\n",
       "      <td>The features failed way too much (like the cha...</td>\n",
       "      <td>feature failed much like chat feature crashed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>tampilan</td>\n",
       "      <td>good app, functioned well minimum crash</td>\n",
       "      <td>good functioned well minimum crash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Rating  Sentimen      aspek  \\\n",
       "0 2018-10-17       5         1    helpful   \n",
       "1 2018-11-12       5         1  pelayanan   \n",
       "2 2018-11-13       2        -1  pelayanan   \n",
       "3 2018-11-13       2        -1   tampilan   \n",
       "4 2018-11-17       5         1   tampilan   \n",
       "\n",
       "                                              Review  \\\n",
       "0                                     Recomended app   \n",
       "1  I have relied on Tokopedia to obtain from chea...   \n",
       "2  What's with OVO shovel treatment? Dissapointed...   \n",
       "3  The features failed way too much (like the cha...   \n",
       "4            good app, functioned well minimum crash   \n",
       "\n",
       "                                      Cleaned_review  \n",
       "0                                        recommended  \n",
       "1  relied cheap mundane item pricy gadget extra c...  \n",
       "2  ovo shovel treatment dissapointed narrow money...  \n",
       "3  feature failed much like chat feature crashed ...  \n",
       "4                 good functioned well minimum crash  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nama_kolom = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates =[]\n",
    "rating=[]\n",
    "sentimen=[]\n",
    "aspek=[]\n",
    "review=[]\n",
    "clean_rv=[]\n",
    "for idx,dt in enumerate(data['Review']):\n",
    "    l_dt = dt.split('.')\n",
    "    for dts in l_dt:\n",
    "        if dts!='':\n",
    "            try:\n",
    "                dates.append(data[nama_kolom[0]][idx]) \n",
    "                rating.append(data[nama_kolom[1]][idx])\n",
    "                sentimen.append(data[nama_kolom[2]][idx])\n",
    "                aspek.append(data[nama_kolom[3]][idx])\n",
    "                review.append(dts)\n",
    "                clean_rv.append(data[nama_kolom[5]][idx])\n",
    "            except Exception as err:\n",
    "                print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti = {'Date':dates,'Rating':rating,'Sentimen':sentimen,'aspek':aspek,'Review':review,'clean_review':clean_rv} \n",
    "new_data = pd.DataFrame(dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>aspek</th>\n",
       "      <th>Review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>helpful</td>\n",
       "      <td>Recomended app</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>I have relied on Tokopedia to obtain from chea...</td>\n",
       "      <td>relied cheap mundane item pricy gadget extra c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>Everything were delivered as expected and I a...</td>\n",
       "      <td>relied cheap mundane item pricy gadget extra c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>Sometimes, Tokopedi</td>\n",
       "      <td>relied cheap mundane item pricy gadget extra c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>pelayanan</td>\n",
       "      <td>Full Review</td>\n",
       "      <td>relied cheap mundane item pricy gadget extra c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Rating  Sentimen      aspek  \\\n",
       "0 2018-10-17       5         1    helpful   \n",
       "1 2018-11-12       5         1  pelayanan   \n",
       "2 2018-11-12       5         1  pelayanan   \n",
       "3 2018-11-12       5         1  pelayanan   \n",
       "4 2018-11-12       5         1  pelayanan   \n",
       "\n",
       "                                              Review  \\\n",
       "0                                     Recomended app   \n",
       "1  I have relied on Tokopedia to obtain from chea...   \n",
       "2   Everything were delivered as expected and I a...   \n",
       "3                                Sometimes, Tokopedi   \n",
       "4                                        Full Review   \n",
       "\n",
       "                                        clean_review  \n",
       "0                                        recommended  \n",
       "1  relied cheap mundane item pricy gadget extra c...  \n",
       "2  relied cheap mundane item pricy gadget extra c...  \n",
       "3  relied cheap mundane item pricy gadget extra c...  \n",
       "4  relied cheap mundane item pricy gadget extra c...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_excel('split_kalimat.xlsx')\n",
    "#file ini udah ke-split tapi kolom sentimen dan aspek blm kosong, jd ngikut kalimat awal isinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isi kolom aspek dan sentimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aslinya yang dilabelin ada 3067, setelah displit kalimat (jumlah baris/data jadi 4425), aspek dan sentimen menyesuaikan label yang sudah diberikan jadi tidak sesuai dengan kalimatnya, makanya diberikan label aspek dan sentimen dari kalimat yang sudah displit dengan model yang sudah dibuat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('C:/WinPython_64bit/notebooks/Google-Play-Store-Review-Extractor-master/split_kalimat 2.xlsx')\n",
    "rev=df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4425,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/4425 [00:00<?, ?it/s]c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "\n",
      "  0%|                                                                                 | 2/4425 [00:00<06:50, 10.78it/s]\n",
      "  0%|                                                                                 | 4/4425 [00:00<06:30, 11.33it/s]\n",
      "  0%|                                                                                 | 6/4425 [00:00<06:26, 11.44it/s]\n",
      "  0%|▏                                                                                | 8/4425 [00:00<06:24, 11.49it/s]\n",
      "  0%|▏                                                                               | 10/4425 [00:00<06:20, 11.62it/s]\n",
      "  0%|▏                                                                               | 12/4425 [00:01<06:19, 11.62it/s]\n",
      "  0%|▎                                                                               | 14/4425 [00:01<06:17, 11.69it/s]\n",
      "  0%|▎                                                                               | 16/4425 [00:01<06:18, 11.66it/s]\n",
      "  0%|▎                                                                               | 18/4425 [00:01<06:16, 11.70it/s]\n",
      "  0%|▎                                                                               | 20/4425 [00:01<06:15, 11.73it/s]\n",
      "  0%|▍                                                                               | 22/4425 [00:01<06:13, 11.78it/s]\n",
      "  1%|▍                                                                               | 24/4425 [00:02<06:13, 11.78it/s]\n",
      "  1%|▍                                                                               | 26/4425 [00:02<06:12, 11.81it/s]\n",
      "  1%|▌                                                                               | 28/4425 [00:02<06:12, 11.81it/s]\n",
      "  1%|▌                                                                               | 30/4425 [00:02<06:11, 11.82it/s]\n",
      "  1%|▌                                                                               | 32/4425 [00:02<06:12, 11.81it/s]\n",
      "  1%|▌                                                                               | 34/4425 [00:02<06:11, 11.82it/s]\n",
      "  1%|▋                                                                               | 36/4425 [00:03<06:11, 11.82it/s]\n",
      "  1%|▋                                                                               | 38/4425 [00:03<06:11, 11.80it/s]\n",
      "  1%|▋                                                                               | 40/4425 [00:03<06:11, 11.82it/s]\n",
      "  1%|▊                                                                               | 42/4425 [00:03<06:10, 11.82it/s]\n",
      "  1%|▊                                                                               | 44/4425 [00:03<06:10, 11.83it/s]\n",
      "  1%|▊                                                                               | 46/4425 [00:03<06:09, 11.84it/s]\n",
      "  1%|▊                                                                               | 48/4425 [00:04<06:09, 11.86it/s]\n",
      "  1%|▉                                                                               | 50/4425 [00:04<06:09, 11.85it/s]\n",
      "  1%|▉                                                                               | 52/4425 [00:04<06:08, 11.87it/s]\n",
      "  1%|▉                                                                               | 54/4425 [00:04<06:08, 11.85it/s]\n",
      "  1%|█                                                                               | 56/4425 [00:04<06:08, 11.84it/s]\n",
      "  1%|█                                                                               | 58/4425 [00:04<06:08, 11.85it/s]\n",
      "  1%|█                                                                               | 60/4425 [00:05<06:08, 11.86it/s]\n",
      "  1%|█                                                                               | 62/4425 [00:05<06:07, 11.86it/s]\n",
      "  1%|█▏                                                                              | 64/4425 [00:05<06:07, 11.87it/s]\n",
      "  1%|█▏                                                                              | 66/4425 [00:05<06:06, 11.88it/s]\n",
      "  2%|█▏                                                                              | 68/4425 [00:05<06:06, 11.88it/s]\n",
      "  2%|█▎                                                                              | 70/4425 [00:05<06:06, 11.88it/s]\n",
      "  2%|█▎                                                                              | 72/4425 [00:06<06:05, 11.90it/s]\n",
      "  2%|█▎                                                                              | 74/4425 [00:06<06:05, 11.91it/s]\n",
      " 16%|████████████▎                                                                  | 691/4425 [01:10<06:20,  9.83it/s]\n",
      "  2%|█▍                                                                              | 82/4425 [00:06<06:04, 11.93it/s]Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4425/4425 [06:12<00:00, 11.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df))):\n",
    "    if pd.isna(df['Sentimen'][i]):\n",
    "        df['Sentimen'][i] = predictSent(rev[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4425 [00:00<?, ?it/s]c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4425/4425 [05:18<00:00, 13.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df))):\n",
    "    if pd.isna(df['aspek'][i]):\n",
    "        df['aspek'][i] = predictAspek(rev[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('hasil.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=5,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=5,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "Tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
    "\n",
    "listdf=listReview.values.astype('U')\n",
    "listdf = [d for d in listdf]\n",
    "\n",
    "tfidf = Tfidf_vectorizer.fit_transform(listdf)\n",
    "tfidf_term = Tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "Pkl_Filename = 'tfidf2.pkl'\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(Tfidf_vectorizer, file)\n",
    "print(Tfidf_vectorizer)\n",
    "\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    vsm = pickle.load(file)\n",
    "print(vsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf\n",
    "y = data['Sentimen'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n",
       "       -1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "        1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,\n",
       "        1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1, -1,\n",
       "       -1,  1,  1, -1, -1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "       -1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1, -1,  1,\n",
       "        1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,\n",
       "        1, -1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1, -1,  1,  1, -1,  1,  1,\n",
       "        1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatif = []\n",
    "positif = []\n",
    "\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "bnb = BernoulliNB()\n",
    "NB = bnb.fit(X_ros, y_ros)\n",
    "y_bnb = bnb.predict(X_test); del bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\winpython_64bit\\python-3.6.5.amd64\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_bnb)):\n",
    "    if y_bnb[i] == -1: \n",
    "        rev[i]=\"negatif\"\n",
    "    else:\n",
    "        rev[i]=\"positif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 memiliki aspek helpful memiliki sentimen 1\n",
      "Review 1 memiliki aspek pengalaman belanja memiliki sentimen -1\n",
      "Review 2 memiliki aspek helpful memiliki sentimen -1\n"
     ]
    }
   ],
   "source": [
    "test_reviews = [\n",
    "    \"Good, fast service.\",\n",
    "    \"get more cashback with many coupons.\",\n",
    "    \"I can not use my coupon while i have right in term and condition for that.\"]\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Review \" + str(i) + \" memiliki aspek \" + predictAspek(rev[i+1]) + \" memiliki sentimen\", predictSent(rev[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
